import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import joblib
from datetime import datetime
import time
import xgboost as xgb

FuelConsumption = pd.read_csv('FuelConsumption.csv')

print(FuelConsumption.head())

X = FuelConsumption[['ENGINESIZE', 'CYLINDERS', 'TRANSMISSION', 'FUELTYPE']]
y = FuelConsumption[['FUELCONSUMPTION_CITY', 'FUELCONSUMPTION_HWY', 'FUELCONSUMPTION_COMB']]

X_encoded = pd.get_dummies(X, columns=['TRANSMISSION', 'FUELTYPE'], drop_first=True)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

param_grid_ridge_lasso = {'alpha': [0.01, 0.1, 1, 10, 100]}
param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
}



ridge = Ridge()
ridge_search = GridSearchCV(ridge, param_grid_ridge_lasso, cv=5, scoring='neg_mean_squared_error')
ridge_search.fit(X_train, y_train)
ridge_best_model = ridge_search.best_estimator_
print(f"Best Ridge Model Alpha: {ridge_search.best_params_}")

lasso = Lasso()
lasso_search = GridSearchCV(lasso, param_grid_ridge_lasso, cv=5, scoring='neg_mean_squared_error')
lasso_search.fit(X_train, y_train)
lasso_best_model = lasso_search.best_estimator_
print(f"Best Lasso Model Alpha: {lasso_search.best_params_}")

rf = RandomForestRegressor(random_state=42)
rf_search = GridSearchCV(rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error')
rf_search.fit(X_train, y_train)
rf_best_model = rf_search.best_estimator_
print(f"Best Random Forest Parameters: {rf_search.best_params_}")

xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
xgb_search = GridSearchCV(xgb_reg, param_grid_xgb, cv=5, scoring='neg_mean_squared_error')
xgb_search.fit(X_train, y_train)
xgb_best_model = xgb_search.best_estimator_
print(f"Best XGBoost Parameters: {xgb_search.best_params_}")

models = [model, ridge_best_model, lasso_best_model, rf_best_model, xgb_best_model]
model_names = ["Linear Regression", "Ridge Regression", "Lasso Regression", "Random Forest", "XGBoost"]

for i, m in enumerate(models):
    predictions = m.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    accuracy_percentage = r2 * 100
    print(f"{model_names[i]} - Mean Squared Error (MSE): {mse:.2f}")
    print(f"{model_names[i]} - Accuracy (R² as %): {accuracy_percentage:.2f}%")
    print("-" * 50)

    plt.scatter(y_test['FUELCONSUMPTION_CITY'], predictions[:, 0], label=f'{model_names[i]}: Predicted vs Actual (City)')
    plt.xlabel('Actual Fuel Consumption (City)')
    plt.ylabel('Predicted Fuel Consumption (City)')
    plt.title(f'{model_names[i]}: Actual vs Predicted Fuel Consumption (City)')
    plt.show()

importances = rf_best_model.feature_importances_
feature_importances = pd.DataFrame({'Feature': X_encoded.columns, 'Importance': importances}).sort_values(by='Importance', ascending=False)
print("Feature Importances (Random Forest):")
print(feature_importances)

perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
perm_importance_df = pd.DataFrame(perm_importance['importances_mean'], index=X_encoded.columns, columns=['Importance'])
print("Permutation Importance (Linear Regression):")
print(perm_importance_df)

xgb_importances = xgb_best_model.feature_importances_
xgb_feature_importances = pd.DataFrame({'Feature': X_encoded.columns, 'Importance': xgb_importances}).sort_values(by='Importance', ascending=False)
print("Feature Importances (XGBoost):")
print(xgb_feature_importances)

def monitor_and_retrain(model_path, performance_threshold=0.1, retrain_interval=60*60*24):
    model = joblib.load(model_path)
    last_retrain_time = datetime.now()

    while True:
        current_time = datetime.now()
        if (current_time - last_retrain_time).total_seconds() > retrain_interval:
            print(f"Retraining model at {current_time}")

            FuelConsumption = pd.read_csv('FuelConsumption.csv')
            X = FuelConsumption[['ENGINESIZE', 'CYLINDERS', 'TRANSMISSION', 'FUELTYPE']]
            y = FuelConsumption[['FUELCONSUMPTION_CITY', 'FUELCONSUMPTION_HWY', 'FUELCONSUMPTION_COMB']]
            X_encoded = pd.get_dummies(X, columns=['TRANSMISSION', 'FUELTYPE'], drop_first=True)
            X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

            model.fit(X_train, y_train)
            predictions = model.predict(X_test)

            mse = mean_squared_error(y_test, predictions)
            r2 = r2_score(y_test, predictions)
            accuracy_percentage = r2 * 100
            print(f"Retrained Model MSE: {mse:.2f}")
            print(f"Retrained Model Accuracy (R² as %): {accuracy_percentage:.2f}%")

            if mse > performance_threshold:
                print("Model performance dropped. Saving retrained model.")
                joblib.dump(model, model_path)
            else:
                print("Model performance acceptable. No need to overwrite.")

            last_retrain_time = current_time
        time.sleep(60 * 60)
